{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp create_project\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing a new project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder structure\n",
    "Assumes the following folder structure. It is important that logger files are named `record_NN.h5` or `record_NNN.h5`\n",
    "\n",
    "Note that, because the data format to communicate parameters used is a Pickle file, the filenames **MUST** be indexed at 0\n",
    "\n",
    "```\n",
    "SIMULATION_NAME/\n",
    "    parameter_grid.json #Describes each run\n",
    "    record_00.h5\n",
    "    record_01.h5\n",
    "    ...\n",
    "    record_NN.h5 # Each logger run\n",
    "    sites.geojson # Polygons of the regions\n",
    "```\n",
    "\n",
    "From this information, you can specify an `output_directory` where summaries will automatically be available for frontend analysis.\n",
    "\n",
    "```\n",
    "public/demo/projects/SIMULATION_NAME/\n",
    "    metadata.json\n",
    "    summary_000.csv\n",
    "    summary_001.csv\n",
    "    ...\n",
    "    summary_NNN.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are provided a `parameter_grid.json` file that looks like the following:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"pub\": [\n",
    "        0.0953169,\n",
    "        0.521456,\n",
    "        0.40569099999999997,\n",
    "        0.484659,\n",
    "        0.138482\n",
    "    ],\n",
    "    \"grocery\": [\n",
    "        0.387384,\n",
    "        0.452953,\n",
    "        0.548852,\n",
    "        0.042028699999999995,\n",
    "        0.21261799999999997\n",
    "    ], ...\n",
    "}\n",
    "```\n",
    "\n",
    "(In this case, there are 5 runs and each run takes the parameter listed. This makes it tricky to do a grid search in the interface since many values will be distinct...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.ops import cascaded_union, unary_union\n",
    "import numpy as np\n",
    "from time import time\n",
    "from typing import *\n",
    "import junevis.path_fixes as pf\n",
    "import json\n",
    "\n",
    "import junevis.process_loggers as process_loggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check available projects\n",
    "\n",
    "Because extracting from the records can take a while, we don't want to overwrite an existing project unless indicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_available_projects(project_name: str):\n",
    "    pf.AVAILABLE_PROJECTS.touch()\n",
    "\n",
    "    with open(str(pf.AVAILABLE_PROJECTS), 'r+') as fp:\n",
    "        available_projects = set([p.strip() for p in fp.readlines()])\n",
    "        if project_name in available_projects:\n",
    "            if not force_add_project:\n",
    "                raise ValueError(f\"Cannot create project of name '{project_name}': Project already exists in {pf.AVAILABLE_PROJECTS}\"\n",
    "    )\n",
    "            else:\n",
    "                shutil.rmtree(outdir) # Delete existing project of that name\n",
    "                fp.truncate(0); fp.seek(0); # Delete file contents\n",
    "                available_projects.remove(project_name)\n",
    "        return available_projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Summary CSVs\n",
    "\n",
    "> Take the `record_**.h5` and convert them to CSVs the frontend can parse\n",
    "\n",
    "These record files can be on the order of 8GB and summarizing each can take about 45 minutes. It works, though it is not the most efficient or parallelized implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def summarize_h5(record_f, outdir):\n",
    "    \"\"\"Dependent on the context variable `output_dir`. The actual summarized output is much smaller than the record file itself\"\"\"\n",
    "    start = time()\n",
    "    runId = record_f.stem.split(\"_\")[1]\n",
    "    print(f\"Processing {runId}: \")\n",
    "    df = process_loggers.regional_outputs(record_f)\n",
    "    \n",
    "    # Add cumulative columns\n",
    "    region_grouped_df = df.groupby(level=0)\n",
    "    df['currently_dead'] = region_grouped_df.deaths.cumsum()\n",
    "    df['currently_recovered'] = region_grouped_df.recovered.cumsum()\n",
    "    \n",
    "    # Rename region\n",
    "    df = df.rename_axis(index=[\"region\", \"timestamp\"])\n",
    "    \n",
    "    outfile = outdir / f\"summary_{int(runId):03}.csv\"\n",
    "    print(f\"Saving to {str(outfile)}\")\n",
    "    df.to_csv(str(outfile))\n",
    "    print(f\"\\nTook {time() - start} seconds\")\n",
    "    print(\"\\n-------\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the `metadata.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to convert the provided `parameter_grid.json` file into a `metadata.json` file (e.g., below) that also includes some basic summary statistics from the project. This has the format: \n",
    "\n",
    "```\n",
    "{\n",
    "    \"description\": \"Learning center comparison\",\n",
    "    \"parameters_varied\": [\n",
    "        \"indoor_beta\",\n",
    "        \"outdoor_beta\",\n",
    "        \"household_beta\",\n",
    "        \"learning_centers\"\n",
    "    ],\n",
    "    \"run_parameters\": {\n",
    "        \"1\": {\n",
    "            \"learning_centers\": false,\n",
    "            \"household_beta\": 0.2,\n",
    "            \"indoor_beta\": 0.45,\n",
    "            \"outdoor_beta\": 0.05\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"learning_centers\": false,\n",
    "            \"household_beta\": 0.2,\n",
    "            \"indoor_beta\": 0.55,\n",
    "            \"outdoor_beta\": 0.05\n",
    "        }, ...\n",
    "    },\n",
    "    \"all_regions\": [\n",
    "        \"CXB-201\",\n",
    "        \"CXB-202\", ...\n",
    "    ], \n",
    "    \"all_timestamps\": [\n",
    "        \"2020-05-01\",\n",
    "        \"2020-05-02\", ...\n",
    "    ], \n",
    "    \"all_fields\": [\n",
    "        \"currently_dead\",\n",
    "        \"currently_in_hospital_0_12\", ...\n",
    "    ],\n",
    "    \"field_statistics\": {\n",
    "        \"n_infections_in_communal\": {\n",
    "            \"max\": 132.0,\n",
    "            \"min\": 0.0\n",
    "        },\n",
    "        \"recovered\": {\n",
    "            \"max\": 1937.0,\n",
    "            \"min\": 0.0\n",
    "        }, ...\n",
    "    }\n",
    "```\n",
    "\n",
    "This involves restructuring the provided parameter grids and parsing the new `summary_**.csvs` for extents of each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pgrid_to_run_parameters(parameter_grid: dict) -> dict:\n",
    "    \"\"\"Convert parameter_grid dictionary to desired metadata dictionary\"\"\"\n",
    "    run_parameters = {}\n",
    "    \n",
    "    # Create run_parameters\n",
    "    for k, v in parameter_grid.items():\n",
    "        for i in range(len(v)):\n",
    "            curr = run_parameters.get(str(i), {})\n",
    "            curr[k] = v[i]\n",
    "            run_parameters[str(i)] = curr\n",
    "    \n",
    "    params_varied = list(parameter_grid.keys())\n",
    "    \n",
    "    return {\n",
    "        \"parameters_varied\": params_varied,\n",
    "        \"run_parameters\": run_parameters,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def collect_statistics(project: Union[str, Path]):\n",
    "    project = Path(project)\n",
    "    csvfs = list(project.glob(\"summary*.csv\"))\n",
    "    dfs = [pd.read_csv(csvf) for csvf in csvfs]\n",
    "    big_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    all_regions = list(set(big_df.region))\n",
    "    all_fields = [f for f in big_df.columns if f != \"Unnamed: 0\"]\n",
    "    all_timestamps = list(set(big_df.timestamp))\n",
    "\n",
    "    string_fields = [\"timestamp\", \"region\", \"Unnamed: 0\"]\n",
    "    numerical_fields = [f for f in all_fields if f not in string_fields]\n",
    "    big_df_num = big_df.loc[:, numerical_fields]\n",
    "    max_vals = big_df_num.max(axis=0)\n",
    "    min_vals = big_df_num.min(axis=0)\n",
    "\n",
    "    df_minmax = pd.DataFrame([max_vals, min_vals], index=[\"max\", \"min\"])\n",
    "    field_minmaxes = df_minmax.to_dict(orient=\"dict\")\n",
    "    \n",
    "    return {\n",
    "        \"all_regions\": sorted(all_regions),\n",
    "        \"all_timestamps\": sorted(all_timestamps),\n",
    "        \"all_fields\": sorted(all_fields),\n",
    "        \"field_statistics\": field_minmaxes\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying the `sites.geojson`\n",
    "\n",
    "This part is a bit simpler. We need to copy the `sites.geojson` file from the provided records to the output directory.\n",
    "\n",
    "Note: some geojson files may be very large. This is the place to reduce the size to something more reasonable yet still functional.\n",
    "\n",
    "Also, some geojson files for this project have been annotated with `SSID` as the 'property' that describes each region. Others are annotated with the `region` key. We need to unify this interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the sites.geojson\n",
    "\n",
    "We need to unify the geojson file a bit. First, the files are terribly large with high resolution (making it very slow to load in the frontend), and the multipolygons are rendering incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fix_geojson(gjson_file):\n",
    "    gdf = gpd.read_file(gjson_file)\n",
    "    \n",
    "    for i, shape in enumerate(gdf.geometry):\n",
    "        # To reduce the shape of the multipolygon, take the shape of the largest area\n",
    "        if shape.geom_type == \"MultiPolygon\":\n",
    "            polygon = shape[np.argmax(np.array([p.area for p in shape]))]\n",
    "        else:\n",
    "            polygon = shape\n",
    "        gdf.geometry[i] = polygon\n",
    "    \n",
    "    # The frontend operates with a `SSID` field instead of a `region` field to name each area.\n",
    "    gdf['SSID'] = gdf['region']\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bundle as Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.script import *\n",
    "\n",
    "@call_parse\n",
    "def main(record_path:Param(\"Path to JUNE simulation records and parameter grid\", str), \n",
    "         force_add_project:Param(\"Overwrite project if it already exists\", store_true)=False,\n",
    "         test_only:Param(\"Test behavior without changing files\", store_true)=False,\n",
    "         project_name:Param(\"Name the project. If not provided, use folder name of `record_path`\", str)=None, \n",
    "         description:Param(\"Description of project\", str)=\"NA\",\n",
    "        ):\n",
    "    \"\"\"Create a project that can be visualized from the record files\"\"\"\n",
    "\n",
    "    base = Path(record_path) # Path where loggers and parameter grid are stored\n",
    "    project_name = base.stem if project_name is None else project_name\n",
    "    output_dir = pf.PROJECTS / project_name\n",
    "    if not output_dir.exists() and not test_only: output_dir.mkdir(parents=True)\n",
    "    \n",
    "    active_projects = init_available_projects(project_name)\n",
    "    \n",
    "    record_names = sorted(list(base.glob(\"*.h5\")))\n",
    "    for r in record_names:\n",
    "        print(f\"Summarizing {r}\")\n",
    "        if not test_only: df = summarize_h5(r, output_dir)\n",
    "\n",
    "    print(\"ALL SUMMARIES COMPLETED\\n-------------\\n-------------\\n\")\n",
    "    \n",
    "    # Once the summary files have been created, we can accumulate the statistics into the `metadata.json` file\n",
    "    print(\"Creating metadata...\")\n",
    "    with open(base / \"parameter_grid.json\") as fp:\n",
    "        parameter_grid = json.load(fp)\n",
    "    param_info = pgrid_to_run_parameters(parameter_grid)\n",
    "    project_stats = collect_statistics(output_dir)\n",
    "    \n",
    "    # Now we can save the metadata for this project, including the optional description\n",
    "    metadata = {\"description\": description}; [metadata.update(p) for p in [param_info, project_stats]];\n",
    "    if not test_only:\n",
    "        with open(output_dir / \"metadata.json\", 'w+') as fp:\n",
    "            json.dump(metadata, fp, indent=4)\n",
    "        \n",
    "    # Copy over the geography description\n",
    "    print(\"Fixing geojson...\")\n",
    "    gdf = fix_geojson(base / \"sites.geojson\")\n",
    "    if not test_only: gdf.to_file(output_dir / \"sites.new.geojson\", driver='GeoJSON')\n",
    "    \n",
    "    # Add to available projects\n",
    "    print(f\"Adding '{project_name}' to {pf.AVAILABLE_PROJECTS}\")\n",
    "    new_available_projects = \"\".join([\"\\n\" + p for p in (list(active_projects) + [project_name])]).strip()\n",
    "    print(f\"New projects: {new_available_projects}\")\n",
    "    \n",
    "    if not test_only:\n",
    "        with open(pf.AVAILABLE_PROJECTS, 'r+') as fp: \n",
    "            fp.write(new_available_projects)\n",
    "            \n",
    "    print(\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_Create Project.ipynb.\n",
      "Converted 01_Tokenizer.ipynb.\n",
      "Converted Collect Global Statistics.ipynb.\n",
      "Converted Scrap for init_project.py.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:june-vis] *",
   "language": "python",
   "name": "conda-env-june-vis-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
